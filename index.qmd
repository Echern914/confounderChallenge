---
title: "Using Randomization & Stratification to Overcome A Common Cause Confounder"
format:
  html: default
bibliography: references.bib
execute:
  echo: false
  eval: true
---



## Solution 1: Randomization — Break the Confounding Path

The gold standard to establish causation is a randomized controlled trial. Instead of letting patients choose their surgeon, we randomly assign them to either Dr. Dreamy or Dr. Duck.

This is exactly the kind of problem randomization can solve. By randomly assigning patients to surgeons, we break the confounding relationship between patient severity and surgeon choice. When assignment is random—rather than driven by patient preferences or availability constraints—we eliminate the confounding pathway entirely. This change, illustrated in @fig-example3-node, allows us to isolate the true causal effect of surgeon identity on outcomes, separate from any influence of patient severity.

![DAG Model Explaining Surgical Outcomes](dag-example3-node.png){#fig-example3-node width=65%}

As shown in @fig-example3-node, randomization fundamentally changes the causal structure. In this scenario, patient severity ($Z$) no longer influences surgeon choice ($X$). Instead, randomization ($R$) completely determines surgeon assignment ($X$). This breaks the confounding relationship, allowing us to assess whether surgeon identity truly matters for surgical outcomes.

### The Counterfactual Problem

::: {.callout-note title="What is a counterfactual?" icon=false}
A *counterfactual* is the answer to a "what if" question: what would have happened if circumstances had been different? Specifically, what would each patient's outcome have been if they had been assigned to the *other* surgeon, holding everything else constant? These are the unobserved alternative outcomes—the outcomes that didn't happen but could have happened under different treatment assignment.
:::

Unfortunately, we can't observe what didn't happen. The patients we already observed went to the surgeon they chose (or who was available), and we'll never know their outcomes under the alternative assignment. This is the fundamental problem of causal inference: we only see one world, not the parallel universe where everything else was held constant but the treatment differed.

Since we cannot rerun history with randomization, we must run an experiment to collect new data. We randomly assigned 100 patients to either Dr. Dreamy or Dr. Duck and observed their outcomes:

```{python}
#| label: simulate-data-randomized
#| echo: false
#| message: false
#| warning: false
#| include: false

import pandas as pd
import numpy as np

# Set seed for reproducibility (different from observational data)
np.random.seed(456)

# Create dataframe with 100 patients and severity scores
n_patients_randomized = 100
patients_randomized_df = (
    pd.DataFrame({
        'patient': range(1, n_patients_randomized + 1),
        'severity': np.random.normal(0, 1, n_patients_randomized)
    })
    .assign(
        # Randomly assign doctors: 50/50 chance, independent of severity
        # This breaks the confounding relationship
        doctor_id=lambda df: np.random.choice([0, 1], size=n_patients_randomized),
        # Assign doctor names based on id
        doctor_name=lambda df: np.where(df['doctor_id'] == 1, 'Doc Dreamy', 'Doc Duck'),
        # Assign surgical goodness:
        # Doc Duck: Normal(0.5, 1), Doc Dreamy: Normal(-0.4, 1)
        # Same true effect as before - Doc Duck is better
        surgicalGoodness=lambda df: np.where(
            df['doctor_name'] == 'Doc Duck',
            np.random.normal(0.5, 1, n_patients_randomized),
            np.random.normal(-0.4, 1, n_patients_randomized)
        ),
        # Calculate post-surgical severity: initial severity - surgical goodness + 3
        post_surgical_score=lambda df: df['severity'] - df['surgicalGoodness'] + 3
    )
)

# Save to CSV, excluding surgicalGoodness column
patients_randomized_df[['patient', 'severity', 'doctor_id', 'doctor_name', 'post_surgical_score']].to_csv(
    'patients_data_randomized.csv', index=False
)

# Display first few rows
patients_randomized_df.head()
```

```{python}
#| label: load-data-randomized-python
#| echo: false

import pandas as pd

# Load the randomized data
patients_randomized_df = pd.read_csv('patients_data_randomized.csv')

# Display first few rows
print(f"Number of randomized patients: {len(patients_randomized_df)}")
patients_randomized_df.head()
```

The results from @fig-plot-outcomes-randomized tell a completely different story. Under randomized assignment, Doc Duck's patients have an average score of 2.71, while Doc Dreamy's patients average 3.46. Since lower scores indicate better outcomes, Doc Duck is clearly the superior surgeon. With randomization breaking the confounding relationship, we can now properly assess the true causal effect. A two-sample t-test confirms this difference is statistically significant (t = 2.734, p = 0.007). 

**The shocking conclusion: Not only is Dr. Dreamy not the better surgeon—he is actually the worse surgeon!** The observational data had it completely backwards.

```{python}
#| label: fig-plot-outcomes-randomized
#| fig-cap: "Post-Surgical Symptom Score (lower is better) for randomized patients."
#| echo: false

import matplotlib.pyplot as plt
import numpy as np

# Separate randomized data by doctor
dreamy_randomized = patients_randomized_df[patients_randomized_df['doctor_name'] == 'Doc Dreamy']
duck_randomized = patients_randomized_df[patients_randomized_df['doctor_name'] == 'Doc Duck']

# Calculate means
dreamy_mean_randomized = dreamy_randomized['post_surgical_score'].mean()
duck_mean_randomized = duck_randomized['post_surgical_score'].mean()

# Create figure with professional styling
fig, ax = plt.subplots(figsize=(7, 4))

# Plot Doc Dreamy (circles, blue)
ax.scatter(dreamy_randomized['patient'], dreamy_randomized['post_surgical_score'], 
           marker='o', color='#4E79A7', s=50, alpha=0.7, 
           label='Doc Dreamy', zorder=3)

# Plot Doc Duck (triangles, coral/red-orange)
ax.scatter(duck_randomized['patient'], duck_randomized['post_surgical_score'], 
           marker='^', color='#E15759', s=50, alpha=0.7, 
           label='Doc Duck', zorder=3)

# Add horizontal dashed lines for means
ax.axhline(y=dreamy_mean_randomized, color='#4E79A7', linestyle='--', linewidth=1.5, 
           alpha=0.7, zorder=2)
ax.axhline(y=duck_mean_randomized, color='#E15759', linestyle='--', linewidth=1.5, 
           alpha=0.7, zorder=2)

# Add text annotations with shape symbols
ax.text(len(patients_randomized_df) * 0.97, dreamy_mean_randomized, f'○ {dreamy_mean_randomized:.2f}', 
        color='#4E79A7', fontsize=11, fontweight='bold',
        verticalalignment='center', horizontalalignment='right',
        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', 
                 edgecolor='#4E79A7', alpha=0.8), zorder=4)

ax.text(len(patients_randomized_df) * 0.97, duck_mean_randomized, f'△ {duck_mean_randomized:.2f}', 
        color='#E15759', fontsize=11, fontweight='bold',
        verticalalignment='center', horizontalalignment='right',
        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', 
                 edgecolor='#E15759', alpha=0.8), zorder=4)

# Styling
ax.set_xlabel('Patient Number', fontsize=12, fontweight='bold')
ax.set_ylabel('Post-Surgical Symptom Score (Lower is Better)', fontsize=12, fontweight='bold')
ax.set_title('Post-Surgical Symptom Score (Randomized Assignment)', 
             fontsize=14, fontweight='bold', pad=15)
ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.5)
ax.legend(loc='upper right', fontsize=11, framealpha=0.9)
ax.set_xlim(-2, len(patients_randomized_df) + 2)

# Adjust layout
plt.tight_layout()
plt.show()

```

## Solution 2: Stratification — When You Can't Randomize

Randomization isn't always possible or ethical. In such cases, we can still handle confounding through stratification. The key is to examine the relationship between treatment and outcome within groups that share the same value of the confounder. In our example, this means looking at patients of similar severity and comparing surgeon outcomes within those groups.

While there are sophisticated mathematical approaches to stratification, we'll demonstrate the concept visually. @fig-plot-outcomes-severity plots patient severity on the x-axis and post-surgical symptom score on the y-axis, revealing how severity affects both surgeon assignment and outcomes.  

```{python}
#| label: fig-plot-outcomes-severity
#| fig-cap: "Post-Surgical Symptom Score (lower is better) by Patient and Severity"
#| echo: false

import matplotlib.pyplot as plt
import numpy as np

# Separate observational data by doctor
dreamy_data = patients_df[patients_df['doctor_name'] == 'Doc Dreamy']
duck_data = patients_df[patients_df['doctor_name'] == 'Doc Duck']

# Calculate means
dreamy_mean = dreamy_data['post_surgical_score'].mean()
duck_mean = duck_data['post_surgical_score'].mean()

# Create figure with professional styling
fig, ax = plt.subplots(figsize=(7, 4))

# Add shaded rectangular overlay highlighting severity between -1 and 1
ax.axvspan(-1, 1, alpha=0.15, color='gray', zorder=0)

# Plot Doc Dreamy (circles, blue)
ax.scatter(dreamy_data['severity'], dreamy_data['post_surgical_score'], 
           marker='o', color='#4E79A7', s=50, alpha=0.7, 
           label='Doc Dreamy', zorder=3)

# Plot Doc Duck (triangles, coral/red-orange)
ax.scatter(duck_data['severity'], duck_data['post_surgical_score'], 
           marker='^', color='#E15759', s=50, alpha=0.7, 
           label='Doc Duck', zorder=3)

# Add horizontal dashed lines for means
ax.axhline(y=dreamy_mean, color='#4E79A7', linestyle='--', linewidth=1.5, 
           alpha=0.7, zorder=2)
ax.axhline(y=duck_mean, color='#E15759', linestyle='--', linewidth=1.5, 
           alpha=0.7, zorder=2)

# Styling
ax.set_xlabel('Initial Patient Severity', fontsize=12, fontweight='bold')
ax.set_ylabel('Post-Surgical Symptom Score (Lower is Better)', fontsize=12, fontweight='bold')
ax.set_title('Post-Surgical Symptom Score by Patient Severity', 
             fontsize=14, fontweight='bold', pad=15)
ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.5)
ax.legend(loc='upper right', fontsize=11, framealpha=0.9)

# Add text annotations with shape symbols for means
# Position annotations on the right side of the plot
x_max = max(patients_df['severity'].max(), abs(patients_df['severity'].min())) * 1.1
ax.text(x_max * 0.95, dreamy_mean, f'○ {dreamy_mean:.2f}', 
        color='#4E79A7', fontsize=11, fontweight='bold',
        verticalalignment='center', horizontalalignment='right',
        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', 
                 edgecolor='#4E79A7', alpha=0.8), zorder=4)

ax.text(x_max * 0.95, duck_mean, f'△ {duck_mean:.2f}', 
        color='#E15759', fontsize=11, fontweight='bold',
        verticalalignment='center', horizontalalignment='right',
        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', 
                 edgecolor='#E15759', alpha=0.8), zorder=4)

# Adjust layout
plt.tight_layout()
plt.show()

```

Examining @fig-plot-outcomes-severity reveals the confounding pattern clearly. The most informative region is between severity scores of -1 and 1 (highlighted in gray), where both surgeons have substantial patient overlap. This overlap provides a fair comparison ground. Outside this range, direct comparisons become difficult: Doc Duck treats more high-severity patients (severity > 1), while Doc Dreamy sees more low-severity patients (severity < -1).

Within the highlighted overlap region, a telling pattern emerges: Doc Dreamy's patients (blue circles) tend to have higher (worse) scores than Doc Duck's patients (red triangles) at similar severity levels. The spatial distribution also confirms the confounding: Doc Duck's patients cluster toward the right (higher severity), while Doc Dreamy's patients cluster toward the left (lower severity).

This visualization reveals what the aggregate statistics hide: when comparing patients of similar severity, Doc Duck consistently outperforms Doc Dreamy. The apparent advantage of Doc Dreamy disappears once we account for patient severity—exactly what we'd expect if severity were a common cause of both surgeon choice and outcomes.

## Conclusion: Defending Against Confounding

This case study illustrates a fundamental challenge in data analysis: numbers can mislead us when we don't understand what they're really measuring. When we simply looked at raw outcomes, Doc Dreamy appeared superior. But that surface-level association masked a deeper truth.

The hidden factor was patient severity—operating like a puppeteer behind the curtain, directing easier cases to Doc Dreamy and harder cases to Doc Duck. Without accounting for this confounder, we couldn't distinguish between "Doc Dreamy is a better surgeon" and "Doc Dreamy got lucky with his patient assignment." This is why correlation doesn't imply causation: our brains are wired to see patterns and assign causes, but we often miss the third variable pulling the strings.

Randomization provides the gold-standard defense. By randomly assigning patients, we break the confounding pathway and reveal the true causal effect. In our randomized experiment, the truth emerged: Doc Duck is actually the better surgeon—the one who doesn't look the part but delivers superior results.

When randomization isn't possible, stratification offers a powerful alternative. By comparing outcomes within groups of similar patient severity, we compare apples to apples rather than apples to oranges. It requires more careful analysis, but it's far better than being misled by confounding.

The takeaway is simple but profound: the world is messy, and data can deceive us if we let it. But by understanding the causal structure—by drawing the DAG and thinking carefully about what causes what—we can defend against being fooled. Use randomization when you can, stratification when you can't. The tools are straightforward; the challenge is remembering to apply them.
