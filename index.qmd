---
title: "Using Randomization & Stratification to Overcome A Common Cause Confounder"
format:
  html: default
bibliography: references.bib
execute:
  echo: false
  eval: true
---

## Introduction

In the world of data analysis and causal inference, things are rarely as straightforward as they appear. What looks like a clear relationship between two variables often masks a more complex story—one where hidden factors pull the strings behind the scenes. This report explores how common-cause confounders can create misleading associations, and demonstrates two powerful tools for uncovering the truth: randomization and stratification.

Through a compelling case study comparing two surgeons, we'll see how observational data can lead us astray, and how proper experimental design and analytical techniques can reveal what's really happening beneath the surface.

## The Paradox: Who *Really* Is the Better Surgeon?

::: {.callout-tip title="A Tale of Two Surgeons" icon=false}
*Paraphrased from* [@taleb2017surgeons]

<div style="float: right; margin-left: 15px; margin-bottom: 10px;">
<img src="docSideBySide.jpg" alt="Doc Dreamy and Doc Duck side by side" style="max-width: 250px; height: auto; border-radius: 5px;">
</div>

Imagine you need to choose between two surgeons of similar rank at the same hospital. The first surgeon, Doc Dreamy, matches our stereotype perfectly: refined appearance, silver-rimmed glasses, delicate hands, measured speech, and an office adorned with Ivy League diplomas (see the image). The second surgeon, Doc Duck, by contrast, looks more like a butcher—overweight, with large hands, an unkempt appearance, and no visible credentials on the wall.

Counterintuitively, the surgeon who doesn't "look the part" may actually be the better choice. Why? Because when someone succeeds in their profession despite not fitting the expected appearance, it suggests they had to overcome significant perceptual biases. And if we are lucky enough to have people who do not look the part, it is thanks to the presence of some skin in the game, the contact with reality that filters out incompetence. [@taleb2017surgeons]
:::

## Observational Data: A Misleading Victory for Doc Dreamy

```{python}
#| label: simulate-data
#| echo: false
#| message: false
#| warning: false
#| include: false

import pandas as pd
import numpy as np

# Set seed for reproducibility
np.random.seed(123)

# Create dataframe with 100 patients and severity scores
n_patients = 100
patients_df = (
    pd.DataFrame({
        'patient': range(1, n_patients + 1),
        'severity': np.random.normal(0, 1, n_patients)
    })
    .assign(
        # Assign doctors based on severity using sigmoid function:
        # Probability of Doc Duck is inversely proportional to severity
        # Higher severity → higher probability of Doc Duck
        # Uses sigmoid: P(Doc Duck) = 1 / (1 + exp(-k * severity))
        # where k controls the steepness of the relationship
        prob_duck=lambda df: 1 / (1 + np.exp(-1.5 * df['severity'])),  # k=1.5 controls sensitivity
        # Random assignment: if random < prob_duck → Doc Duck (id=0), else Doc Dreamy (id=1)
        doctor_id=lambda df: (np.random.random(n_patients) >= df['prob_duck']).astype(int),
        # Assign doctor names based on id
        doctor_name=lambda df: np.where(df['doctor_id'] == 1, 'Doc Dreamy', 'Doc Duck'),
        # Assign surgical goodness:
        # Doc Duck: Normal(0.2, 1), Doc Dreamy: Normal(-0.2, 1)
        surgicalGoodness=lambda df: np.where(
            df['doctor_name'] == 'Doc Duck',
            np.random.normal(0.5, 1, n_patients),
            np.random.normal(-0.4, 1, n_patients)
        ),
        # Calculate post-surgical severity: initial severity - surgical goodness + 3
        post_surgical_score=lambda df: df['severity'] - df['surgicalGoodness'] + 3
    )
)

# Save to CSV, excluding prob_duck and surgicalGoodness columns
patients_df[['patient', 'severity', 'doctor_id', 'doctor_name', 'post_surgical_score']].to_csv(
    'patients_data.csv', index=False
)

# Display first few rows
patients_df.head()
```

```{python}
#| label: load-data-python
#| echo: false

import pandas as pd

# Load the observational data
patients_df = pd.read_csv('patients_data.csv')

# Display first few rows to understand the structure
print(f"Number of patients: {len(patients_df)}")
patients_df.head()
```

To determine which surgeon performs better, we analyzed post-surgical symptom scores for 100 patients. As shown in @fig-plot-outcomes, Doc Dreamy's patients have an average score of 2.8, while Doc Duck's patients average 3.38. Since lower scores indicate better outcomes, Doc Dreamy appears to be the superior surgeon. A two-sample t-test confirms this difference is statistically significant (t = -2.317, p = 0.023).

At first glance, the conclusion seems clear: choose Doc Dreamy. But as we'll see, this initial assessment misses a crucial piece of the puzzle. 

```{python}
#| label: fig-plot-outcomes
#| fig-cap: "Post-Surgical Symptom Score (lower is better) - Observed Data"
#| echo: false

import matplotlib.pyplot as plt
import numpy as np

# Separate data by doctor
dreamy_data = patients_df[patients_df['doctor_name'] == 'Doc Dreamy']
duck_data = patients_df[patients_df['doctor_name'] == 'Doc Duck']

# Calculate means
dreamy_mean = dreamy_data['post_surgical_score'].mean()
duck_mean = duck_data['post_surgical_score'].mean()

# Create figure with professional styling
fig, ax = plt.subplots(figsize=(7, 4))

# Plot Doc Dreamy (circles, blue)
ax.scatter(dreamy_data['patient'], dreamy_data['post_surgical_score'], 
           marker='o', color='#4E79A7', s=50, alpha=0.7, 
           label='Doc Dreamy', zorder=3)

# Plot Doc Duck (triangles, coral/red-orange)
ax.scatter(duck_data['patient'], duck_data['post_surgical_score'], 
           marker='^', color='#E15759', s=50, alpha=0.7, 
           label='Doc Duck', zorder=3)

# Add horizontal dashed lines for means
ax.axhline(y=dreamy_mean, color='#4E79A7', linestyle='--', linewidth=1.5, 
           alpha=0.7, zorder=2)
ax.axhline(y=duck_mean, color='#E15759', linestyle='--', linewidth=1.5, 
           alpha=0.7, zorder=2)

# Add text annotations with shape symbols
ax.text(len(patients_df) * 0.97, dreamy_mean, f'○ {dreamy_mean:.2f}', 
        color='#4E79A7', fontsize=11, fontweight='bold',
        verticalalignment='center', horizontalalignment='right',
        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', 
                 edgecolor='#4E79A7', alpha=0.8), zorder=4)

ax.text(len(patients_df) * 0.97, duck_mean, f'△ {duck_mean:.2f}', 
        color='#E15759', fontsize=11, fontweight='bold',
        verticalalignment='center', horizontalalignment='right',
        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', 
                 edgecolor='#E15759', alpha=0.8), zorder=4)

# Styling
ax.set_xlabel('Patient Number', fontsize=12, fontweight='bold')
ax.set_ylabel('Post-Surgical Symptom Score (Lower is Better)', fontsize=12, fontweight='bold')
ax.set_title('Post-Surgical Symptom Score by Patient and Doctor', 
             fontsize=14, fontweight='bold', pad=15)
ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.5)
ax.legend(loc='upper right', fontsize=11, framealpha=0.9)
ax.set_xlim(-2, len(patients_df) + 2)

# Adjust layout
plt.tight_layout()
plt.show()

```

## The Hidden Confounder: Patient Severity Explains It All

When we look at @fig-plot-outcomes, our brains naturally gravitate toward the simplest explanation: the mental model shown in @fig-example-node.

![DAG Model Explaining Surgical Outcomes](dag-example-node.png){#fig-example-node width=50%}

The obvious conclusion from using the mental model of @fig-example-node and the data shown in @fig-plot-outcomes is that **Doc Dreamy is the superior surgeon because his patients' scores are lower than Doc Duck's patients' scores**.

However, a "common cause" confounder can create an association between two variables that isn't actually causal. Consider a classic example: if we observe puddles on the road ($X$) and people carrying umbrellas ($Y$), we might mistakenly conclude that puddles cause people to carry umbrellas. But the real explanation is simpler: rain ($Z$) causes both puddles and umbrella-carrying. This creates a spurious correlation that masks the true causal structure.

As Nassim Taleb suggests, there may be a similar explanation in our surgeon study. Consider the model shown in @fig-example2-node. Could there be a hidden common cause driving the association between surgeon choice and surgical outcomes?

![DAG Model Explaining Surgical Outcomes](dag-example2-node.png){#fig-example2-node width=50%}

Here's what might actually be happening. Both surgeons maintain full schedules, but with different booking timelines: Dr. Dreamy schedules surgeries 3 weeks in advance, while Dr. Duck can accommodate patients within 1 week. This creates a natural sorting mechanism: patients with lower severity, who aren't in urgent need, can wait for Dr. Dreamy's availability and are more likely to choose him based on his polished online presence. Meanwhile, patients with higher severity, who need immediate care, gravitate toward Dr. Duck simply because he's available sooner—often the only surgeon who can see them right away.

::: {.callout-warning title="Assumption: Slow Progression of Patient Severity" icon=false}
For simplicity, we assume that patient severity progresses slowly enough that a 2-week delay (i.e., waiting for Dr. Dreamy's availability) has zero effect on surgical outcomes. This delay affects only how quickly patients receive surgical relief, not the eventual outcome itself. While time-to-surgery can be an important factor in other contexts, here we focus solely on whether initial patient severity might create a spurious or biased association between surgeon choice and outcomes.
:::

## Solution 1: Randomization — Break the Confounding Path

The gold standard to establish causation is a randomized controlled trial. Instead of letting patients choose their surgeon, we randomly assign them to either Dr. Dreamy or Dr. Duck.

This is exactly the kind of problem randomization can solve. By randomly assigning patients to surgeons, we break the confounding relationship between patient severity and surgeon choice. When assignment is random—rather than driven by patient preferences or availability constraints—we eliminate the confounding pathway entirely. This change, illustrated in @fig-example3-node, allows us to isolate the true causal effect of surgeon identity on outcomes, separate from any influence of patient severity.

![DAG Model Explaining Surgical Outcomes](dag-example3-node.png){#fig-example3-node width=65%}

As shown in @fig-example3-node, randomization fundamentally changes the causal structure. In this scenario, patient severity ($Z$) no longer influences surgeon choice ($X$). Instead, randomization ($R$) completely determines surgeon assignment ($X$). This breaks the confounding relationship, allowing us to assess whether surgeon identity truly matters for surgical outcomes.

### The Counterfactual Problem

::: {.callout-note title="What is a counterfactual?" icon=false}
A *counterfactual* is the answer to a "what if" question: what would have happened if circumstances had been different? Specifically, what would each patient's outcome have been if they had been assigned to the *other* surgeon, holding everything else constant? These are the unobserved alternative outcomes—the outcomes that didn't happen but could have happened under different treatment assignment.
:::

Unfortunately, we can't observe what didn't happen. The patients we already observed went to the surgeon they chose (or who was available), and we'll never know their outcomes under the alternative assignment. This is the fundamental problem of causal inference: we only see one world, not the parallel universe where everything else was held constant but the treatment differed.

Since we cannot rerun history with randomization, we must run an experiment to collect new data. We randomly assigned 100 patients to either Dr. Dreamy or Dr. Duck and observed their outcomes:

```{python}
#| label: simulate-data-randomized
#| echo: false
#| message: false
#| warning: false
#| include: false

import pandas as pd
import numpy as np

# Set seed for reproducibility (different from observational data)
np.random.seed(456)

# Create dataframe with 100 patients and severity scores
n_patients_randomized = 100
patients_randomized_df = (
    pd.DataFrame({
        'patient': range(1, n_patients_randomized + 1),
        'severity': np.random.normal(0, 1, n_patients_randomized)
    })
    .assign(
        # Randomly assign doctors: 50/50 chance, independent of severity
        # This breaks the confounding relationship
        doctor_id=lambda df: np.random.choice([0, 1], size=n_patients_randomized),
        # Assign doctor names based on id
        doctor_name=lambda df: np.where(df['doctor_id'] == 1, 'Doc Dreamy', 'Doc Duck'),
        # Assign surgical goodness:
        # Doc Duck: Normal(0.5, 1), Doc Dreamy: Normal(-0.4, 1)
        # Same true effect as before - Doc Duck is better
        surgicalGoodness=lambda df: np.where(
            df['doctor_name'] == 'Doc Duck',
            np.random.normal(0.5, 1, n_patients_randomized),
            np.random.normal(-0.4, 1, n_patients_randomized)
        ),
        # Calculate post-surgical severity: initial severity - surgical goodness + 3
        post_surgical_score=lambda df: df['severity'] - df['surgicalGoodness'] + 3
    )
)

# Save to CSV, excluding surgicalGoodness column
patients_randomized_df[['patient', 'severity', 'doctor_id', 'doctor_name', 'post_surgical_score']].to_csv(
    'patients_data_randomized.csv', index=False
)

# Display first few rows
patients_randomized_df.head()
```

```{python}
#| label: load-data-randomized-python
#| echo: false

import pandas as pd

# Load the randomized data
patients_randomized_df = pd.read_csv('patients_data_randomized.csv')

# Display first few rows
print(f"Number of randomized patients: {len(patients_randomized_df)}")
patients_randomized_df.head()
```

The results from @fig-plot-outcomes-randomized tell a completely different story. Under randomized assignment, Doc Duck's patients have an average score of 2.71, while Doc Dreamy's patients average 3.46. Since lower scores indicate better outcomes, Doc Duck is clearly the superior surgeon. With randomization breaking the confounding relationship, we can now properly assess the true causal effect. A two-sample t-test confirms this difference is statistically significant (t = 2.734, p = 0.007). 

**The shocking conclusion: Not only is Dr. Dreamy not the better surgeon—he is actually the worse surgeon!** The observational data had it completely backwards.

```{python}
#| label: fig-plot-outcomes-randomized
#| fig-cap: "Post-Surgical Symptom Score (lower is better) for randomized patients."
#| echo: false

import matplotlib.pyplot as plt
import numpy as np

# Separate randomized data by doctor
dreamy_randomized = patients_randomized_df[patients_randomized_df['doctor_name'] == 'Doc Dreamy']
duck_randomized = patients_randomized_df[patients_randomized_df['doctor_name'] == 'Doc Duck']

# Calculate means
dreamy_mean_randomized = dreamy_randomized['post_surgical_score'].mean()
duck_mean_randomized = duck_randomized['post_surgical_score'].mean()

# Create figure with professional styling
fig, ax = plt.subplots(figsize=(7, 4))

# Plot Doc Dreamy (circles, blue)
ax.scatter(dreamy_randomized['patient'], dreamy_randomized['post_surgical_score'], 
           marker='o', color='#4E79A7', s=50, alpha=0.7, 
           label='Doc Dreamy', zorder=3)

# Plot Doc Duck (triangles, coral/red-orange)
ax.scatter(duck_randomized['patient'], duck_randomized['post_surgical_score'], 
           marker='^', color='#E15759', s=50, alpha=0.7, 
           label='Doc Duck', zorder=3)

# Add horizontal dashed lines for means
ax.axhline(y=dreamy_mean_randomized, color='#4E79A7', linestyle='--', linewidth=1.5, 
           alpha=0.7, zorder=2)
ax.axhline(y=duck_mean_randomized, color='#E15759', linestyle='--', linewidth=1.5, 
           alpha=0.7, zorder=2)

# Add text annotations with shape symbols
ax.text(len(patients_randomized_df) * 0.97, dreamy_mean_randomized, f'○ {dreamy_mean_randomized:.2f}', 
        color='#4E79A7', fontsize=11, fontweight='bold',
        verticalalignment='center', horizontalalignment='right',
        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', 
                 edgecolor='#4E79A7', alpha=0.8), zorder=4)

ax.text(len(patients_randomized_df) * 0.97, duck_mean_randomized, f'△ {duck_mean_randomized:.2f}', 
        color='#E15759', fontsize=11, fontweight='bold',
        verticalalignment='center', horizontalalignment='right',
        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', 
                 edgecolor='#E15759', alpha=0.8), zorder=4)

# Styling
ax.set_xlabel('Patient Number', fontsize=12, fontweight='bold')
ax.set_ylabel('Post-Surgical Symptom Score (Lower is Better)', fontsize=12, fontweight='bold')
ax.set_title('Post-Surgical Symptom Score (Randomized Assignment)', 
             fontsize=14, fontweight='bold', pad=15)
ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.5)
ax.legend(loc='upper right', fontsize=11, framealpha=0.9)
ax.set_xlim(-2, len(patients_randomized_df) + 2)

# Adjust layout
plt.tight_layout()
plt.show()

```

## Solution 2: Stratification — When You Can't Randomize

Randomization isn't always possible or ethical. In such cases, we can still handle confounding through stratification. The key is to examine the relationship between treatment and outcome within groups that share the same value of the confounder. In our example, this means looking at patients of similar severity and comparing surgeon outcomes within those groups.

While there are sophisticated mathematical approaches to stratification, we'll demonstrate the concept visually. @fig-plot-outcomes-severity plots patient severity on the x-axis and post-surgical symptom score on the y-axis, revealing how severity affects both surgeon assignment and outcomes.  

```{python}
#| label: fig-plot-outcomes-severity
#| fig-cap: "Post-Surgical Symptom Score (lower is better) by Patient and Severity"
#| echo: false

import matplotlib.pyplot as plt
import numpy as np

# Separate observational data by doctor
dreamy_data = patients_df[patients_df['doctor_name'] == 'Doc Dreamy']
duck_data = patients_df[patients_df['doctor_name'] == 'Doc Duck']

# Calculate means
dreamy_mean = dreamy_data['post_surgical_score'].mean()
duck_mean = duck_data['post_surgical_score'].mean()

# Create figure with professional styling
fig, ax = plt.subplots(figsize=(7, 4))

# Add shaded rectangular overlay highlighting severity between -1 and 1
ax.axvspan(-1, 1, alpha=0.15, color='gray', zorder=0)

# Plot Doc Dreamy (circles, blue)
ax.scatter(dreamy_data['severity'], dreamy_data['post_surgical_score'], 
           marker='o', color='#4E79A7', s=50, alpha=0.7, 
           label='Doc Dreamy', zorder=3)

# Plot Doc Duck (triangles, coral/red-orange)
ax.scatter(duck_data['severity'], duck_data['post_surgical_score'], 
           marker='^', color='#E15759', s=50, alpha=0.7, 
           label='Doc Duck', zorder=3)

# Add horizontal dashed lines for means
ax.axhline(y=dreamy_mean, color='#4E79A7', linestyle='--', linewidth=1.5, 
           alpha=0.7, zorder=2)
ax.axhline(y=duck_mean, color='#E15759', linestyle='--', linewidth=1.5, 
           alpha=0.7, zorder=2)

# Styling
ax.set_xlabel('Initial Patient Severity', fontsize=12, fontweight='bold')
ax.set_ylabel('Post-Surgical Symptom Score (Lower is Better)', fontsize=12, fontweight='bold')
ax.set_title('Post-Surgical Symptom Score by Patient Severity', 
             fontsize=14, fontweight='bold', pad=15)
ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.5)
ax.legend(loc='upper right', fontsize=11, framealpha=0.9)

# Add text annotations with shape symbols for means
# Position annotations on the right side of the plot
x_max = max(patients_df['severity'].max(), abs(patients_df['severity'].min())) * 1.1
ax.text(x_max * 0.95, dreamy_mean, f'○ {dreamy_mean:.2f}', 
        color='#4E79A7', fontsize=11, fontweight='bold',
        verticalalignment='center', horizontalalignment='right',
        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', 
                 edgecolor='#4E79A7', alpha=0.8), zorder=4)

ax.text(x_max * 0.95, duck_mean, f'△ {duck_mean:.2f}', 
        color='#E15759', fontsize=11, fontweight='bold',
        verticalalignment='center', horizontalalignment='right',
        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', 
                 edgecolor='#E15759', alpha=0.8), zorder=4)

# Adjust layout
plt.tight_layout()
plt.show()

```

Examining @fig-plot-outcomes-severity reveals the confounding pattern clearly. The most informative region is between severity scores of -1 and 1 (highlighted in gray), where both surgeons have substantial patient overlap. This overlap provides a fair comparison ground. Outside this range, direct comparisons become difficult: Doc Duck treats more high-severity patients (severity > 1), while Doc Dreamy sees more low-severity patients (severity < -1).

Within the highlighted overlap region, a telling pattern emerges: Doc Dreamy's patients (blue circles) tend to have higher (worse) scores than Doc Duck's patients (red triangles) at similar severity levels. The spatial distribution also confirms the confounding: Doc Duck's patients cluster toward the right (higher severity), while Doc Dreamy's patients cluster toward the left (lower severity).

This visualization reveals what the aggregate statistics hide: when comparing patients of similar severity, Doc Duck consistently outperforms Doc Dreamy. The apparent advantage of Doc Dreamy disappears once we account for patient severity—exactly what we'd expect if severity were a common cause of both surgeon choice and outcomes.

## Conclusion: Defending Against Confounding

This case study illustrates a fundamental challenge in data analysis: numbers can mislead us when we don't understand what they're really measuring. When we simply looked at raw outcomes, Doc Dreamy appeared superior. But that surface-level association masked a deeper truth.

The hidden factor was patient severity—operating like a puppeteer behind the curtain, directing easier cases to Doc Dreamy and harder cases to Doc Duck. Without accounting for this confounder, we couldn't distinguish between "Doc Dreamy is a better surgeon" and "Doc Dreamy got lucky with his patient assignment." This is why correlation doesn't imply causation: our brains are wired to see patterns and assign causes, but we often miss the third variable pulling the strings.

Randomization provides the gold-standard defense. By randomly assigning patients, we break the confounding pathway and reveal the true causal effect. In our randomized experiment, the truth emerged: Doc Duck is actually the better surgeon—the one who doesn't look the part but delivers superior results.

When randomization isn't possible, stratification offers a powerful alternative. By comparing outcomes within groups of similar patient severity, we compare apples to apples rather than apples to oranges. It requires more careful analysis, but it's far better than being misled by confounding.

The takeaway is simple but profound: the world is messy, and data can deceive us if we let it. But by understanding the causal structure—by drawing the DAG and thinking carefully about what causes what—we can defend against being fooled. Use randomization when you can, stratification when you can't. The tools are straightforward; the challenge is remembering to apply them.
